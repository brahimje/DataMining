{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brahimje/DataMining/blob/main/ImageMining/Atelier4_FeaturesEngineering_Brahim_JELLITE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5enhfiSt2VLx"
      },
      "source": [
        "<center> <h1>Ingénierie des caractéristiques <br> (Features engineering)</h1></center>\n",
        "\n",
        "Pour regarder la vidéo du support de cours:\n",
        "https://youtu.be/OTEhvJhismM\n",
        "\n",
        "A la fin, il faut remplir le formulaire ci-dessous utilisant les données de votre implémentation\n",
        "\n",
        "https://forms.gle/1jmXku2ftAehbceRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0ZZ2Hwdzez3"
      },
      "source": [
        "<h1> Plan </h1>\n",
        "\n",
        "*\tIntroduction\n",
        "*\tNormalisation des caractéristiques\n",
        "*\tSélection des caractéristiques\n",
        "   -\tRéduction\n",
        "   -  Sélection\n",
        "*\tImplémentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNs-cVLa0Y3S"
      },
      "source": [
        "<h1> Introduction </h1>\n",
        "Concevoir un modèle de classification performant repose sur plusieurs facteurs:\n",
        "\n",
        "* Type et cohérence de la base données (base de données équilibrées)\n",
        "* Choix des caractéristiques pertinentes\n",
        "* Choix d’un algorithme de classification judicieux\n",
        "\n",
        "D’après les experts dans le domaine:\n",
        "\n",
        "* Les caractéristiques à utiliser en classification influencent plus que tout autre paramètre sur le résultat. \n",
        "* Aucun algorithme ne peut remplacer à lui seul le supplément que les caractéristiques procurent\n",
        "\n",
        "Selon une enquête de Forbes (2016), les datascientists  consacrent 80 % (19% collection; 60% nettoyage et organisation) de leur temps à la préparation des données. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA-qglwY03HC"
      },
      "source": [
        "<h1>Ingénierie de caractéristiques</h1>\n",
        "L'Ingénierie de caractéristiques consiste en le traitement des caractéristiques dans le but d'améliorer le comportement du modéle de classification. <br>\n",
        "Plusieurs étapes peuvent être utilisées (pas toutes utilisables pour l’image mining) \n",
        "\n",
        "* Valeurs manquantes (Non rencotrés en Image Mining)\n",
        "* Valeurs aberrantes\n",
        "* Transformation logarithmique\n",
        "* Encodage unique (Concerne uniquement les étiquettes en Image Mining)\n",
        "* Mise en échelle\n",
        "* Sélection des caractéristiques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6fblqPN1-VW"
      },
      "source": [
        "<h1> Implémentation </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7Maj0p2PHo"
      },
      "source": [
        "<h2> Préparation de l'environnement</h2>\n",
        "Nous allons travailler dans cet atelier sur des caractéristiques déja extraites et enregistrées sur Google Drive  (Voir atelier tranfer Learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az36GUAH63ZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a2dc15-7436-4499-f6ac-5b504b918e0d"
      },
      "source": [
        "#from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKCruVyWOig"
      },
      "source": [
        "<h2> Chargement de la dataset</h2>\n",
        "Pour tester et appliquer l'ingénierie de caractéristiques, nous allons utiliser des caractéristiques déjà extraites utilisant une architecture Deep Learning VGG16 de la base d'image Corel utilisée dans l'atelier de classification supervisée. \n",
        "Pour télécharger le fichier de caractéristiques et les étiquettes cliquer sur les liens ci-dessous :\n",
        "\n",
        "[Features](https://drive.google.com/file/d/105zEqcYD1Deuzy5NM5dOPqnXDETUAfVt/view?usp=share_link)\n",
        "\n",
        "\n",
        "[Labels](https://drive.google.com/file/d/1aggw9QXm9ao7CEmLa4ign9xi2zovUPnB/view?usp=drivesdk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlrbCdv5216j"
      },
      "source": [
        "Vous pouvez utiliser le même chemin que celui que j'ai utilisé :\n",
        "/content/drive/MyDrive/FeaturesEngineering/\n",
        "- Créer un dossier sur la racine de votre drive \"MyDrive\" et nommé le: \"FeaturesEngineering\"\n",
        "- Copier les fichiers \"features_vgg16\" et \"labels\" dans le dossier \"FeaturesEngineering\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Charger les caractéristiques et les étiquettes.</h3>"
      ],
      "metadata": {
        "id": "sDSBKCVrxuCp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mocd_cxfmnF",
        "outputId": "ff16226d-8327-4b53-8d3c-351b203b867b"
      },
      "source": [
        "# Load features and labels\n",
        "import pickle\n",
        "fetauresPath='/content/drive/MyDrive/FeaturesEngineering/'\n",
        "X = pickle.load( open( fetauresPath+\"features_vgg16\", \"rb\" ) )\n",
        "y =pickle.load( open( fetauresPath+\"labels\", \"rb\" ) )\n",
        "\n",
        "import numpy as np\n",
        "print(np.array(X).shape, np.array(y).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 1000) (490,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQR4kVA93NJz"
      },
      "source": [
        "Pour chacune des étapes de l'ingénierie de caractéristiques, nous allons évaluer à chaque fois la classification afin de voir l'impact. \n",
        "Compléter l'implémentation de la fonction de classification. Utiliser un algorithme de classification de votre choix. \n",
        "La méthode doit afficher le taux de classification (accuracy) ainsi que le temps d'exécution de classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8te-Q8eUcKZA"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "def classify(X,y):\n",
        "  # Compléter le code\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  \n",
        "  accuracy = clf.score(X_test, y_test)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  scores = cross_val_score(clf, X, y, cv=5)\n",
        "\n",
        "  # affichez les scores de chaque itération de la validation croisée\n",
        "  print(\"Cross validation scores:\", scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13dK8hRi4PY0"
      },
      "source": [
        "Classification utilisant les caractéristiques initiales sans manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDSvx4pV4Oal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67401d5e-046c-4d18-9d7c-ad1f249aaf84"
      },
      "source": [
        "# Afficher l'accuracy\n",
        "classify(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9489795918367347\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DM0ri2E4CUQ"
      },
      "source": [
        "<h1>Mise en échelle</h1>\n",
        "Dans la plupart des cas, les caractéristiques de type numérique ne sont pas dans la même plage d’intervalle.\n",
        "Exemple : les données sur les âges et les salaires différent et ne n’ont pas la même fourchette.\n",
        "Pour un algorithme d’apprentissage automatique, les valeurs qui ne changent pas beaucoup même sur différentes échelles vont être très influentes en termes de classification.\n",
        "Les algorithmes de classification basés sur le voisinage ou distance sont très sensible à la mise en échelle ; comme le KNN, K-means, …\n",
        "Pour mettre en échelle les données nous avons deux solutions :\n",
        "\n",
        "1.   Normalisation : min-max normalization\n",
        "2.   Standardisation : z-score normalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMAS9qf7cAk"
      },
      "source": [
        "<h2> Standarization: z-score normalization</h2>\n",
        "<center>z=(X-µ)/σ\n",
        "\n",
        "X est l'ensemble d'apprentissage, µ est la moyenne et  σ est l’écart type\n",
        "</center>\n",
        "Le z-score met en échelle les valeurs en prenant en compte l’écart type (déviation standard). <br> \n",
        "le z-score réduit l’effet des données aberrantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmxT5qUn4AqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671be80b-8a6c-45d8-bff5-5c02fee553dc"
      },
      "source": [
        "\"\"\"Standarization\n",
        "Standardize features by removing the mean and scaling to unit variance.\n",
        "z = (x - u) / s\n",
        "where u is the mean of the training samples or zero if with_mean=False\n",
        "s is the standard deviation of the training samples or one if with_std=False.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def standardize(X):\n",
        "    # Calculate the mean and standard deviation of the input data\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "\n",
        "    # Standardize the data by subtracting the mean and dividing by the standard deviation\n",
        "    z = (X - mean) / std\n",
        "    return z\n",
        "\n",
        "# Xzscore est la matrice de caractéristiques après standardisation\n",
        "Xzscore = standardize(X)\n",
        "#print(np.array(Xzscore).shape)\n",
        "classify(Xzscore,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9387755102040817\n",
            "Cross validation scores: [0.98979592 0.96938776 0.98979592 0.94897959 0.93877551]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR_TAfUF8fEm"
      },
      "source": [
        "<h2>Normalization: min-max normalization</h2>\n",
        "<center>Xnorm=(X-Xmin)/(Xmax -Xmin)</center>\n",
        "Permet de mettre en échelle toutes les valeurs des caractéristiques dans une plage fixe entre 0 et 1.<br>\n",
        "L'inconvénient de la normalization par min-max est qu'elle augmente les effets des valeurs aberrantes.<br>\n",
        "Ainsi, avant la normalisation, il est recommandé de traiter les valeurs aberrantes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtDmpKJT9TaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48af9a4-20b3-461d-d8da-16b9bef73d16"
      },
      "source": [
        "\"\"\"Noramlization\n",
        "Xnorm=(X-Xmin)/(Xmax -Xmin).\n",
        "\"\"\"\n",
        "def normalize(X):\n",
        "    # Calculate the minimum and maximum values of the input data\n",
        "    Xmin = np.min(X, axis=0)\n",
        "    Xmax = np.max(X, axis=0)\n",
        "\n",
        "    # Normalize the data by subtracting the minimum value and dividing by the range\n",
        "    Xnorm = (X - Xmin) / (Xmax - Xmin)\n",
        "\n",
        "    return Xnorm\n",
        "\n",
        "# Xminmax est la matrice de caractéristiques après normalisation\n",
        "Xminmax = normalize(X)\n",
        "\n",
        "#print(np.array(Xminmax).shape)\n",
        "classify(Xminmax,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9285714285714286\n",
            "Cross validation scores: [0.96938776 0.96938776 0.96938776 0.97959184 0.95918367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvSAQajiRpSD"
      },
      "source": [
        "<h2> Transformation logarithmique</h2>\n",
        "Pour éliminer les données aberrantes, une des techniques les plus utilisées est la transformation logarithmique.\n",
        "\n",
        "*\tTraiter des données biaisées et, après transformation, la distribution devient plus proche de la normale.\n",
        "*\tRéduire l’ordre de grandeur des données. Exemple : la différence de taille n’est pas de la même grandeur que la différence d'âges\n",
        "*\tRéduit aussi l'effet des valeurs aberrantes grâce à la normalisation des différences d'amplitude\n",
        "\n",
        "NB : il ne faut pas que les données soient négatives\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzaegQ7dS-b_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184ac612-05c7-4363-eda9-3d1a8c8a54cc"
      },
      "source": [
        "# Transformation logarithmique\n",
        "# Xlog est la matrice de caractéristiques après transformation logarithmique\n",
        "Xlog= np.log(X)\n",
        "# Classification utilisant les caractéristiques aprés tranformation logarithmique\n",
        "classify(Xlog,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9795918367346939\n",
            "Cross validation scores: [1.         0.98979592 0.98979592 0.98979592 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Par la suite nous allons procéder a la mise en échelle des données après transformation logarithmique"
      ],
      "metadata": {
        "id": "jTnv_Uuqru_7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_tnub6sBz5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178f1bc8-d1dc-4b44-dd9d-c9e014a488d9"
      },
      "source": [
        "# Normalization aprés TL\n",
        "def normalize_after_log_transform(X):\n",
        "    # Apply the logarithmic transformation to each element in the input data\n",
        "    X_transformed = np.log(X)\n",
        "\n",
        "    # Calculate the minimum and maximum values of the transformed data\n",
        "    Xmin = np.min(X_transformed, axis=0)\n",
        "    Xmax = np.max(X_transformed, axis=0)\n",
        "\n",
        "    # Normalize the transformed data by subtracting the minimum value and dividing by the range\n",
        "    Xnorm = (X_transformed - Xmin) / (Xmax - Xmin)\n",
        "\n",
        "    return Xnorm\n",
        "\n",
        "Xminmax_Log = normalize_after_log_transform(X)\n",
        "\n",
        "classify(Xminmax_Log,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9795918367346939\n",
            "Cross validation scores: [1.         0.98979592 0.98979592 0.98979592 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GF77nX0Sz1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e19530-0aa0-4e73-d8de-39f3a56cf7f0"
      },
      "source": [
        "# Standarization + TL\n",
        "def standardize_and_log_transform(X):\n",
        "    X_transformed = np.log(X)\n",
        "    X_standardized = standardize(X)\n",
        "    return X_standardized\n",
        "\n",
        "Xzscore_Log = standardize_and_log_transform(X)\n",
        "classify(Xzscore_Log,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9387755102040817\n",
            "Cross validation scores: [0.98979592 0.96938776 0.98979592 0.94897959 0.93877551]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzEa3D0hvgho"
      },
      "source": [
        "<h1>Sélection de caractéristiques (features selection)</h1>\n",
        "La sélection de caractéristiques (features selection) est l’opération permettant de d’extraire intelligemment de la connaissance à partir des données brutes. \n",
        "\n",
        "Objectif:\n",
        "* Améliorer les performances (en termes de rapidité, de puissance de prédiction et de la simplicité du modèle).\n",
        "* Réduire les dimensions et éliminer le bruit. \n",
        "\n",
        "La sélection des caractéristiques est un processus qui choisit un sous-ensemble optimal de caractéristique en fonction d'un critère donné.\n",
        "\n",
        "Objectif de FS: <br>\n",
        "* Supprimer les données non pertinentes,\n",
        "* Augmenter de la précision du modèle d’apprentissage notamment en réduisant les besoins en stockage et les coûts de calcul,\n",
        "* Réduire le nombre de données,\n",
        "* Réduire la complexité de la description du modèle résultant,\n",
        "* Améliorer la compréhension des données et du modèle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCfHsnNdF2vG"
      },
      "source": [
        "<h2>Les catégories de la sélection des caractéristiques</h2>\n",
        "Trois approches sont généralement utilisées dans la sélection des caractéristiques : embedded, wrapper et filter: <br>\n",
        "\n",
        "* Les méthodes « embedded » intègrent directement la sélection dans le processus d’apprentissage. Les arbres de décision est une illustration de ce système. \n",
        "* Les méthodes « wrapper » optimisent explicitement un critère de précision, le plus souvent le taux d’erreur. Elles ne s’appuient en rien sur les caractéristiques de l’algorithme d’apprentissage qui est utilisé comme une boîte noire.\n",
        "* les méthodes « filter » agissent en amont, avant la mise en œuvre de la technique d’apprentissage, et sans lien direct avec celui‐ci.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnUqnVBhytSl"
      },
      "source": [
        "<h2> Suppression des caractéristiques à faible variance</h2>\n",
        "Suppression des caractéristiques à faible variance (Removing features with low variance) est une méthode de sélection (élimination) basée sur le filtrage.<br>\n",
        "On peut la considérer comme une méthode de nettoyage de caractéristique qui  élémine toutes les caractéristiques dont la variance n'atteint pas un certain seuil.<br>\n",
        "Par défaut, elle supprime toutes les caractéristiques à variance nulle, c'est-à-dire les caractéristiques qui ont la même valeur dans tous les échantillons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0iwZjCy7X8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f69342b-2603-44c6-f230-1256ee4f8030"
      },
      "source": [
        "# Nous allons utiliser les caractéristiques après normalisation Min-Max\n",
        "# Xvth est la matrice de caractéristiques après suppression des caractéristiques (après normalisation Min-Max)  à faible variance\n",
        "def remove_low_variance_features(X, threshold=0.1):\n",
        "    \n",
        "    Xnorm = normalize(X)\n",
        "\n",
        "    # Calculate the variance of each feature\n",
        "    variance = np.var(Xnorm, axis=0)\n",
        "\n",
        "    # Select the features with variance above the threshold\n",
        "    X_filtered = X[:, variance > threshold]\n",
        "\n",
        "    return X_filtered\n",
        "\n",
        "Xvth = remove_low_variance_features(X, threshold=0.001)\n",
        "classify(Xvth,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9489795918367347\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgWGA_pFzams"
      },
      "source": [
        "<h2>chi2</h2>\n",
        "C’est un algorithme de sélection des caractéristiques qui appartient à la famille des algorithmes de filtrage basé sur la statistique 𝜒2. Cette méthode mesure l’écart à l’indépendance entre une caractéristique et une classe. Elle commence par un niveau de signification élevé pour toutes les caractéristiques pour la discrétisation et chaque caractéristique est triée en fonction de ses valeurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2jeNK2Izhlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee155b00-c85e-4b19-b51c-20544288ff95"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def select_kbest_features(X, y, k=100):\n",
        "    # Use the chi-squared test to select the k best features\n",
        "    selector = SelectKBest(chi2, k=k)\n",
        "    X_new = selector.fit_transform(X, y)\n",
        "\n",
        "    return X_new\n",
        "\n",
        "#Xchi2 est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant chi2\n",
        "Xchi2 = select_kbest_features(X, y, k=100)\n",
        "classify(Xchi2,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9489795918367347\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEsfySe00s_Y"
      },
      "source": [
        "<h2>Recursive Feature Elimination (RFE)</h2>\n",
        "C’est une méthode de cartographie basée sur l'idée à plusieurs reprises, construire un modèle et choisir le meilleur ou le pire performant. Cette méthode qui appartient aux méthodes de filtrage est souvent utilisée comme étape de prétraitement pour les méthodes intégrée (souvent avec l’algorithme de classification SVM) afin de la généraliser à des grandes masses de données\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D2nZnux09jD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018b6203-1dd2-432c-92b5-9ce4d34bbde3"
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "rfe = RFE(model, n_features_to_select = 100)\n",
        "rfe = rfe.fit(X, y)\n",
        "\n",
        "# Xrfe est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant RFE\n",
        "Xrfel = rfe.transform(X)\n",
        "classify(Xrfel,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9489795918367347\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BygazAk9mo_C"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-IqzWEcjpFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88b0338-13b3-40d4-fe83-4d468cdefb61"
      },
      "source": [
        "# Code pour calculer le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification\n",
        "ranking = rfe.ranking_\n",
        "\n",
        "max_score = 0\n",
        "best_num_features = 0\n",
        "for num_features in range(1, len(ranking) + 1):\n",
        "    # Select the top-ranked features\n",
        "    mask = ranking <= num_features\n",
        "    X_selected = X[:, mask]\n",
        "\n",
        "    # Evaluate the classification rate using cross-validation\n",
        "    scores = cross_val_score(SVC(kernel='linear'), X_selected, y, cv=5)\n",
        "    score = np.mean(scores)\n",
        "\n",
        "    # Update the maximum score and the number of features that gave the maximum score\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        best_num_features = num_features\n",
        "\n",
        "print('The maximum classification rate is', max_score)\n",
        "print('It is achieved with', best_num_features, 'features')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum classification rate is 0.9734693877551021\n",
            "It is achieved with 1 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlfaMjkGhMNM"
      },
      "source": [
        "<h1>Relief</h1>\n",
        "Tente de déterminer le plus proche voisin d'un certain nombre d'échantillons sélectionnés au hasard à partir de l'ensemble de données. Pour chaque échantillon sélectionné, les valeurs des caractéristiques sont comparées à ceux des voisins les plus proches et les scores pour chaque caractéristique sont mis à jour. L'idée est d'estimer la qualité des attributs en fonction de la qualité de leurs valeurs et faire la distinction entre des échantillons proches les uns des autres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOUjTkg5jjHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624ef13b-9080-43b1-93bc-a94e6bc6daaf"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "selector = SelectKBest(mutual_info_classif, k=100)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# XRelief est la matrice de caractéristiques après sélection de 100 caractéristiques utilisant Relief\n",
        "XRelief= X_new\n",
        "classify(XRelief,y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9387755102040817\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JURQ4cvq7-s6"
      },
      "source": [
        "Essayer d'identifier le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xJR__tO7-s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26491ee-b62b-49dd-e9c9-cbfb19209b04"
      },
      "source": [
        "# Code pour calculer le nombre minimal de caractéristiques à utiliser pour obtenir le taux maximal de classification\n",
        "ranking = selector.scores_\n",
        "max_score = 0\n",
        "best_num_features = 0\n",
        "for num_features in range(1, len(ranking) + 1):\n",
        "    # Select the top-ranked features\n",
        "    mask = ranking <= num_features\n",
        "    X_selected = X[:, mask]\n",
        "\n",
        "    # Evaluate the classification rate using cross-validation\n",
        "    scores = cross_val_score(SVC(kernel='linear'), X_selected, y, cv=5)\n",
        "    score = np.mean(scores)\n",
        "\n",
        "    # Update the maximum score and the number of features that gave the maximum score\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        best_num_features = num_features\n",
        "\n",
        "print('The maximum classification rate is', max_score)\n",
        "print('It is achieved with', best_num_features, 'features')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum classification rate is 0.9734693877551021\n",
            "It is achieved with 1 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85yhbzTsfwDj"
      },
      "source": [
        "<h1>Réduction de la dimensionnalité </h1>\n",
        "La réduction de la dimensionnalité transforme les caractéristiques en une dimension inférieure. Elle peut être considérée comme étant une méthode de  Selection ou de création (extraction) de caractéristiques où nous dérivons des informations à partir de l’ensemble de caractéristiques de base pour construire un nouveau sous espace de caractéristiques. <br>\n",
        "Ls approches de réduction de dimensionnalité les plus connues sont: PCA (Principal Component Analysis; Analyse en Composantes Principales (ACP)) et ICA (Independent Component Analysis, Analyse en Composantes Independantes (ACI)) \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCqFtKhxl5v7"
      },
      "source": [
        "<h2> ACP </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u1-zsXzviSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dc6ff7-9ee5-4e14-a626-e3d3ea085093"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=100)\n",
        "\n",
        "# Xpca est la matrice de caractéristiques après réduction de la dimensionalité utilisant l'ACP\n",
        "# Essayer de changer la taille de réduction de l'ACP afin d'obtenir le meilleur taux de classification\n",
        "Xpca=pca.fit_transform(X)\n",
        "print(np.array(Xpca).shape)\n",
        "classify(Xpca,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 100)\n",
            "Accuracy: 0.9489795918367347\n",
            "Cross validation scores: [0.96938776 0.97959184 0.98979592 0.97959184 0.94897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dehxuzuHl9Jo"
      },
      "source": [
        "<h2> ACI </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNw7JwD0mCoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8dbf6b-d023-4814-c130-686d5b915f05"
      },
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "ica = FastICA(n_components=300)\n",
        "\n",
        "# Xica est la matrice de caractéristiques après réduction de la dimensionalité utilisant l'ACI\n",
        "# Essayer de changer la taille de réduction de l'ACI afin d'obtenir le meilleur taux de classification\n",
        "Xica = ica.fit_transform(X)\n",
        "print(np.array(Xica).shape)\n",
        "classify(Xica,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(490, 300)\n",
            "Accuracy: 0.7653061224489796\n",
            "Cross validation scores: [0.85714286 0.82653061 0.82653061 0.83673469 0.83673469]\n"
          ]
        }
      ]
    }
  ]
}