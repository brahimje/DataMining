{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgwaplJYFX5Zu4GC4YMqWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brahimje/DataMining/blob/main/Basic_train_set_preprocessing_text_mining_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pshCQG0aB1av"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic train set preprocessing with TensorFlow\n",
        "train_list_sentences = [\n",
        "    'I love my country',\n",
        "    'I love my university',\n",
        "    'You love my country',\n",
        "    'Do you think my country and my university are wonderul ?'\n",
        "]\n",
        "\n",
        "print('\\n train_list_sentences (raw ssentences) :\\n', train_list_sentences)\n",
        "\n",
        "# Create a Tokenizer object\n",
        "# Check tokenizer class (lower, filter .....)\n",
        "#\n",
        "tk = Tokenizer(oov_token='<OOV>')\n",
        "\n",
        "# Fit tokenizer on the train_list_sentences (documents)\n",
        "tk.fit_on_texts(train_list_sentences)\n",
        "\n",
        "# Once fit, the tokenizer provides 4 attriutes that you can use to query wht has been learned about your document\n",
        "# Summarize what was learned\n",
        "print(\"\\n tokenizer.word_index = vocabulary : \\n\", tk.index_word)\n",
        "print(\"\\n len(tokenizer.word_index) = vocal size: \\n\", len(tk.word_index))\n",
        "print(\"\\n tokenizer.word_count : \\n\", tk.word_counts)\n",
        "print(\"\\n tokenizer.word_docs : \\n\", tk.word_docs)\n",
        "print(\"\\n tokenizer.document_count : \\n\", tk.document_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wNnPOb9CFYq",
        "outputId": "ade2b161-2324-431c-b9dd-ce35c3bfed80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " train_list_sentences (raw ssentences) :\n",
            " ['I love my country', 'I love my university', 'You love my country', 'Do you think my country and my university are wonderul ?']\n",
            "\n",
            " tokenizer.word_index = vocabulary : \n",
            " {1: '<OOV>', 2: 'my', 3: 'love', 4: 'country', 5: 'i', 6: 'university', 7: 'you', 8: 'do', 9: 'think', 10: 'and', 11: 'are', 12: 'wonderul'}\n",
            "\n",
            " len(tokenizer.word_index) = vocal size: \n",
            " 12\n",
            "\n",
            " tokenizer.word_count : \n",
            " OrderedDict([('i', 2), ('love', 3), ('my', 5), ('country', 3), ('university', 2), ('you', 2), ('do', 1), ('think', 1), ('and', 1), ('are', 1), ('wonderul', 1)])\n",
            "\n",
            " tokenizer.word_docs : \n",
            " defaultdict(<class 'int'>, {'love': 3, 'i': 2, 'country': 3, 'my': 4, 'university': 2, 'you': 2, 'and': 1, 'do': 1, 'wonderul': 1, 'think': 1, 'are': 1})\n",
            "\n",
            " tokenizer.document_count : \n",
            " 4\n"
          ]
        }
      ]
    }
  ]
}